{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8YDToDCsryC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import os\n",
        "import cv2\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRxsXqaV0jRJ",
        "outputId": "b86f90c4-fd32-4ef0-efcc-c4d5d6742044"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOloXpIY06Ok",
        "outputId": "81d089cc-d611-4c20-8047-c5f03786cc54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file created successfully.\n"
          ]
        }
      ],
      "source": [
        "path = r'/content/drive/My Drive/trimmed_waveform_1'\n",
        "csv_filename = 'file_status.csv'\n",
        "\n",
        "# Get list of files in the directory\n",
        "files = os.listdir(path)\n",
        "\n",
        "# Create CSV file and write headers\n",
        "with open(csv_filename, 'w', newline='') as csvfile:\n",
        "    fieldnames = ['real_value']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writeheader()\n",
        "\n",
        "    # Write 1 if filename starts with 'L', otherwise write 0\n",
        "\n",
        "    for i in glob.glob(path + \"/L*.npy\"):\n",
        "      writer.writerow({'real_value': 1})\n",
        "    for i in glob.glob(path + \"/*.npy\"):\n",
        "      writer.writerow({'real_value': 0})\n",
        "\n",
        "print(\"CSV file created successfully.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUNi4Byr9qVd"
      },
      "outputs": [],
      "source": [
        "lensed_waveform = []  # Initialize lists to store filenames\n",
        "unlensed_waveform = []\n",
        "\n",
        "for i in glob.glob(path + \"/L*.npy\"):\n",
        "    lensed_waveform.append(i)\n",
        "\n",
        "for i in glob.glob(path + \"/*.npy\"):\n",
        "    if not any(i == lwf for lwf in lensed_waveform):\n",
        "        unlensed_waveform.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iH8IcDk29rwp",
        "outputId": "5a8760e0-e32a-4807-944b-fda76cc59f1a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "len(lensed_waveform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fmo8BxCG9yWs",
        "outputId": "55e6c261-4fdb-49d0-fb00-fecdb4aa5234"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "len(unlensed_waveform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wIvhb0-944A"
      },
      "outputs": [],
      "source": [
        "lensed_waveform = [np.load(file) for file in lensed_waveform]\n",
        "unlensed_waveform = [np.load(file) for file in unlensed_waveform]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oBtwQKy95vi"
      },
      "outputs": [],
      "source": [
        "desired_shape = (614, 1)\n",
        "lensed_waveform = [np.resize(waveform, desired_shape) for waveform in lensed_waveform]\n",
        "unlensed_waveform = [np.resize(waveform, desired_shape) for waveform in unlensed_waveform]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiukrIvgIUZ8"
      },
      "outputs": [],
      "source": [
        "# Created labels\n",
        "lensed_labels = np.ones(len(lensed_waveform))\n",
        "unlensed_labels = np.zeros(len(unlensed_waveform))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHgzAxq7i2Qz"
      },
      "outputs": [],
      "source": [
        "# Combine data and labels\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = np.concatenate([lensed_waveform, unlensed_waveform])\n",
        "y = np.concatenate([lensed_labels, unlensed_labels])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming your data is in X and y variables\n",
        "# X represents the features, and y represents the target variable\n",
        "\n",
        "# Split the data into training and testing sets (80% training, 20% test+validation)\n",
        "X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Further split the test+validation set into test and validation sets (50% test, 50% validation)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALy2q6MK-gjr"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDvdSzhjK8Nw",
        "outputId": "4bf974b8-d5af-4f99-c5ad-bb6fdd62bd7b"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "500/500 [==============================] - 528s 1s/step - loss: 0.6915 - accuracy: 0.5056 - val_loss: 0.6932 - val_accuracy: 0.4805\n",
            "Epoch 2/30\n",
            "500/500 [==============================] - 513s 1s/step - loss: 0.6932 - accuracy: 0.5023 - val_loss: 0.6942 - val_accuracy: 0.4805\n",
            "Epoch 3/30\n",
            "500/500 [==============================] - 508s 1s/step - loss: 0.6932 - accuracy: 0.5013 - val_loss: 0.6932 - val_accuracy: 0.4805\n",
            "Epoch 4/30\n",
            "500/500 [==============================] - 503s 1s/step - loss: 0.6932 - accuracy: 0.4973 - val_loss: 0.6932 - val_accuracy: 0.4805\n",
            "Epoch 5/30\n",
            "500/500 [==============================] - 498s 996ms/step - loss: 0.6932 - accuracy: 0.4923 - val_loss: 0.6932 - val_accuracy: 0.4805\n",
            "Epoch 6/30\n",
            "500/500 [==============================] - 500s 1s/step - loss: 0.6932 - accuracy: 0.5008 - val_loss: 0.6934 - val_accuracy: 0.4805\n",
            "Epoch 7/30\n",
            "500/500 [==============================] - 503s 1s/step - loss: 0.6932 - accuracy: 0.4973 - val_loss: 0.6932 - val_accuracy: 0.4805\n",
            "Epoch 8/30\n",
            "500/500 [==============================] - 505s 1s/step - loss: 0.6932 - accuracy: 0.4972 - val_loss: 0.6932 - val_accuracy: 0.4805\n",
            "Epoch 9/30\n",
            "500/500 [==============================] - 493s 987ms/step - loss: 0.6932 - accuracy: 0.4989 - val_loss: 0.6932 - val_accuracy: 0.4805\n",
            "Epoch 10/30\n",
            "500/500 [==============================] - 500s 1s/step - loss: 0.6932 - accuracy: 0.4969 - val_loss: 0.6932 - val_accuracy: 0.4805\n",
            "Epoch 11/30\n",
            "500/500 [==============================] - 489s 978ms/step - loss: 0.6932 - accuracy: 0.4961 - val_loss: 0.6933 - val_accuracy: 0.4805\n",
            "Epoch 12/30\n",
            "500/500 [==============================] - 499s 997ms/step - loss: 0.6932 - accuracy: 0.4991 - val_loss: 0.6934 - val_accuracy: 0.4805\n",
            "Epoch 13/30\n",
            "500/500 [==============================] - 497s 995ms/step - loss: 0.6932 - accuracy: 0.5012 - val_loss: 0.6932 - val_accuracy: 0.4805\n",
            "Epoch 14/30\n",
            "500/500 [==============================] - 505s 1s/step - loss: 0.6932 - accuracy: 0.4986 - val_loss: 0.6933 - val_accuracy: 0.4805\n",
            "Epoch 15/30\n",
            "500/500 [==============================] - 499s 998ms/step - loss: 0.6932 - accuracy: 0.4993 - val_loss: 0.6932 - val_accuracy: 0.4805\n",
            "Epoch 16/30\n",
            "500/500 [==============================] - 492s 985ms/step - loss: 0.6932 - accuracy: 0.4994 - val_loss: 0.6931 - val_accuracy: 0.4805\n",
            "Epoch 17/30\n",
            "500/500 [==============================] - 505s 1s/step - loss: 0.6932 - accuracy: 0.4977 - val_loss: 0.6933 - val_accuracy: 0.4805\n",
            "Epoch 18/30\n",
            "500/500 [==============================] - 501s 1s/step - loss: 0.6932 - accuracy: 0.4966 - val_loss: 0.6933 - val_accuracy: 0.4805\n",
            "Epoch 19/30\n",
            "500/500 [==============================] - 496s 993ms/step - loss: 0.6932 - accuracy: 0.5012 - val_loss: 0.6933 - val_accuracy: 0.4805\n",
            "Epoch 20/30\n",
            "500/500 [==============================] - 502s 1s/step - loss: 0.6932 - accuracy: 0.4943 - val_loss: 0.6933 - val_accuracy: 0.4805\n",
            "Epoch 21/30\n",
            "500/500 [==============================] - 503s 1s/step - loss: 0.6932 - accuracy: 0.5008 - val_loss: 0.6933 - val_accuracy: 0.4805\n",
            "Epoch 22/30\n",
            "500/500 [==============================] - 490s 980ms/step - loss: 0.6932 - accuracy: 0.4991 - val_loss: 0.6931 - val_accuracy: 0.5195\n",
            "Epoch 23/30\n",
            "500/500 [==============================] - 496s 993ms/step - loss: 0.6932 - accuracy: 0.4996 - val_loss: 0.6930 - val_accuracy: 0.5195\n",
            "Epoch 24/30\n",
            "500/500 [==============================] - 496s 993ms/step - loss: 0.6932 - accuracy: 0.4946 - val_loss: 0.6931 - val_accuracy: 0.5195\n",
            "Epoch 25/30\n",
            "500/500 [==============================] - 496s 991ms/step - loss: 0.6932 - accuracy: 0.4927 - val_loss: 0.6931 - val_accuracy: 0.5195\n",
            "Epoch 26/30\n",
            "500/500 [==============================] - 498s 997ms/step - loss: 0.6932 - accuracy: 0.5016 - val_loss: 0.6934 - val_accuracy: 0.4805\n",
            "Epoch 27/30\n",
            "500/500 [==============================] - 500s 1s/step - loss: 0.6932 - accuracy: 0.5003 - val_loss: 0.6931 - val_accuracy: 0.5195\n",
            "Epoch 28/30\n",
            "500/500 [==============================] - 494s 989ms/step - loss: 0.6932 - accuracy: 0.4926 - val_loss: 0.6931 - val_accuracy: 0.5195\n",
            "Epoch 29/30\n",
            "500/500 [==============================] - 500s 1s/step - loss: 0.6932 - accuracy: 0.4982 - val_loss: 0.6933 - val_accuracy: 0.4805\n",
            "Epoch 30/30\n",
            "500/500 [==============================] - 510s 1s/step - loss: 0.6932 - accuracy: 0.4998 - val_loss: 0.6935 - val_accuracy: 0.4805\n",
            "63/63 [==============================] - 14s 214ms/step - loss: 0.6930 - accuracy: 0.5100\n",
            "63/63 [==============================] - 14s 219ms/step\n",
            "CSV file created successfully.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "from keras import models, layers\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Assuming X_train, X_val, y_train, y_val, X_test, y_test are already defined\n",
        "\n",
        "# Function to classify files\n",
        "def classify_file(model, data):\n",
        "    # Predict the class using the model\n",
        "    prediction = model.predict(data)\n",
        "\n",
        "    # Round the prediction to get binary classification result (0 or 1)\n",
        "    return np.round(prediction).astype(int)\n",
        "\n",
        "# Load the trained model\n",
        "model = models.Sequential()\n",
        "model.add(layers.Input(shape=(614, 1)))\n",
        "model.add(layers.Conv1D(filters=256, kernel_size=2, activation='relu'))\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Conv1D(filters=256, kernel_size=2, activation='relu'))\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Conv1D(filters=128, kernel_size=2, activation='relu'))\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Conv1D(filters=128, kernel_size=2, activation='relu'))\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Conv1D(filters=64, kernel_size=2, activation='relu'))\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Conv1D(filters=64, kernel_size=2, activation='relu'))\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Conv1D(filters=64, kernel_size=2, activation='relu'))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "model.add(layers.Dense(7656,activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer=Adam(lr=0.00001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_val, y_val))\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "# Predictions for test data\n",
        "test_predictions = classify_file(model, X_test)\n",
        "\n",
        "# Create CSV file and write predictions\n",
        "csv_filename = 'final_data.csv'\n",
        "with open(csv_filename, 'w', newline='') as csvfile:\n",
        "    fieldnames = ['real_value', 'pred_data']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writeheader()\n",
        "\n",
        "    # Write 0 for the first 10000 files\n",
        "   # for pred in test_predictions:\n",
        "     # writer.writerow({'real_value': 0,'pred_data': pred })\n",
        "\n",
        "    # Write predictions for the last 10000 files\n",
        "       # for pred in test_predictions:\n",
        "          #  writer.writerow({'pred_data': pred})\n",
        "\n",
        "           # writer.writerow({'real_value': 1, 'pred_data': pred})\n",
        "\n",
        "\n",
        "    # Write predictions for the last 10000 files\n",
        "    for pred in test_predictions:\n",
        "        writer.writerow({'pred_data': pred})\n",
        "\n",
        "print(\"CSV file created successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDYNn97uGpTy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eca81920-e948-481c-fefa-fb2596e3e57a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True Positives (TP): 19990\n",
            "True Negatives (TN): 1\n",
            "False Positives (FP): 0\n",
            "False Negatives (FN): 9\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "# Open the CSV file for reading\n",
        "with open('final_data.csv', 'r', newline='') as csvfile:\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    tp = 0\n",
        "    tn = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "    for row in reader:\n",
        "        # Convert string values to integers\n",
        "        real_value = bool(row['real_value'])\n",
        "        pred_data = bool(row['pred_data'])\n",
        "\n",
        "\n",
        "        # Increment tp, tn, fp, or fn based on conditions\n",
        "\n",
        "        if real_value == 1 and pred_data == True:\n",
        "            tp += 1\n",
        "        elif real_value == 0 and pred_data == False:\n",
        "            tn += 1\n",
        "        elif real_value == 0 and pred_data == True:\n",
        "            fp += 1\n",
        "        elif real_value == 1 and pred_data == False:\n",
        "            fn += 1\n",
        "\n",
        "\n",
        "print(\"True Positives (TP):\", tp)\n",
        "print(\"True Negatives (TN):\", tn)\n",
        "print(\"False Positives (FP):\", fp)\n",
        "print(\"False Negatives (FN):\", fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAaM1mjUHxv1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5db873e-d099-49fc-a2ae-ea768c6568c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy= 99.955\n",
            "recall= 99.9549977498875\n"
          ]
        }
      ],
      "source": [
        "print(\"accuracy=\", (tp+tn)*100/float(tp+tn+fn+fp))\n",
        "print(\"recall=\", tp*100/float(tp+fn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnTCkyK8JnR7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "202fe433-0037-4c2c-cfa7-f55e089f303f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1_score= 99.97749381079797\n"
          ]
        }
      ],
      "source": [
        "precision = (tp*100)/float(tp+fp);\n",
        "recall= tp*100/float(tp+fn)\n",
        "f1_score = (2*precision*recall)/(precision+recall)\n",
        "print(\"f1_score=\",f1_score)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}